# Deep Reinforcement Learning for Autonomous Driving

Link --> https://arxiv.org/pdf/1811.11329v1.pdf

## [Abstract]  
강화 학습이 게임 분야에서 많이 사용되고 있음. 그러나 현실 세계에서는 observation space가 complex하고,  
action space가 continuous하면서 세밀한 제어가 필요하기 때문에 자율주행 자동차 분야에서는 쉽지 않음.  
또한 자율 주행 자동차는 어느 정도 안전성을 보장해야 함. 따라서 이 논문에서는 위의 문제들을 해결하기 위해서  
TORCS 환경에서 DDPG 알고리즘을 이용하여 complex한 observation space와 action space를 해결하려고 함.  
보상 함수를 새로 디자인해서 충돌을 방지하려고 함. Test 환경은 Torcs의 다른 환경에서 진행.  

## [제안 방법]  
DDPG알고리즘을 이용해서 TORCS 시뮬레이터에 적용함.
TORCS에서 18가지 센서 데이터를 제공하는데 그 중 사용한 센서는 ob.angle, ob.track, ob.trackpos, ob.speedX, ob.speedY, ob.speedZ.  
보상함수의 목표는 궤도 축을 따라 빠른 속도일 경우 높은 보상 값을 주고, 궤도 축에 수직인 경우 낮은 보상 값을 준다.  

수식 = V_x(cos(theta)) - alpha* V_x(sin(theta)) - gamma*|trackPos| - beta * V_x * |trackPos|
  
### 모델 구조  
1. Actor Network
input 노드의 개수 29, 첫 번째 hidden layer의 노드 개수 300, 두 번째 hidden layer 노드 개수 600, output 노드 개수 3(accel, brake, steer)  

2. Critic Network
Hidden Layer [300, 600, 600]  
첫 번째 히든 레이어의 결과 노드와 액터 네트워크의 결과를 concat해서 두 번째 히든 레이어부터 입력으로 넣음.


## [실험]  
실험 세팅  
TORCS 시뮬레이터를 사용.  
아래 링크에서 기본 하이퍼 파라미터 가져옴.  
 https://yanpanlau.github.io/2016/10/11/Torcs-Keras.html. 
replay buffer - 100000, discount factor - 0.99, optimizer - Adam, learning rate - 0.0001(Actor) and 0.001(Critic)  
batch size - 32, target update tau - 0.001


실험 분석  
실험에 사용한 맵은 Aalborg, 테스트에 사용한 맵은 9대의 차량이 있는 Competition mode.  
에피소드의 종료 조건은 차량이 부딪치거나 차량의 방향이 돌아간 경우. 따라서 잘하는 에이전트의 경우는 최대 60000 step.  
아래 링크는 결과 동영상임.
https://www.dropbox.com/s/balm1vlajjf50p6/drive4.mov?dl=0  
  
결과 그래프는 논문을 참고.  
## [결론]  
적절한 센서 데이터와 최적화된 보상 함수를 설정할 수 있다면 심층 강화학습으로 자율주행 자동차를 학습 시킬 수 있다.
